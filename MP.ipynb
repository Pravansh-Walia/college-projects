{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "EMUMTb9t1Mxa",
        "outputId": "58f5bf22-4da6-435d-9a33-2bf6291fa085"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a7801ff4ab23>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the dataset from Kaggle link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/Resume(1).csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace ';' with the actual delimiter used in your file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Resume(1).csv'"
          ]
        }
      ],
      "source": [
        "  import pandas as pd\n",
        "\n",
        "# Load the dataset from Kaggle link\n",
        "file_path='/Resume(1).csv'\n",
        "df = pd.read_csv(file_path)  # Replace ';' with the actual delimiter used in your file.\n",
        "\n",
        "\n",
        "# Display the first few rows of the dataset to inspect its structure\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# Define the file path to your dataset\n",
        "file_path = 'Resume(2).csv'  # Replace with your file path\n",
        "\n",
        "# Initialize an empty list to store valid rows\n",
        "valid_rows = []\n",
        "\n",
        "# Open the CSV file and handle errors\n",
        "try:\n",
        "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "\n",
        "        for row in csvreader:\n",
        "            try:\n",
        "                # Attempt to append the row to the list\n",
        "                valid_rows.append(row)\n",
        "            except csv.Error as e:\n",
        "                print(f\"CSV Error in row {csvreader.line_num}: {e}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found: {file_path}\")\n",
        "\n",
        "# Convert the list of valid rows to a DataFrame\n",
        "df = pd.DataFrame(valid_rows)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxqxJC-B3l2W",
        "outputId": "218851e4-09b5-4aca-988f-a6ddca7d8c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        0                                                  1    \\\n",
            "0        ID                                         Resume_str   \n",
            "1  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
            "2  22323967           HR SPECIALIST, US HR OPERATIONS      ...   \n",
            "3  33176873           HR DIRECTOR       Summary      Over 2...   \n",
            "4  27018550           HR SPECIALIST       Summary    Dedica...   \n",
            "\n",
            "                                                 2         3   4   5   6    \\\n",
            "0                                        Resume_html  Category               \n",
            "1  <div class=\"fontsize fontface vmargins hmargin...        HR               \n",
            "2  <div class=\"fontsize fontface vmargins hmargin...        HR               \n",
            "3  <div class=\"fontsize fontface vmargins hmargin...        HR               \n",
            "4  <div class=\"fontsize fontface vmargins hmargin...        HR               \n",
            "\n",
            "  7   8   9    ... 159 160 161 162 163 164 165 166 167 168  \n",
            "0              ...                                          \n",
            "1              ...                                          \n",
            "2              ...                                          \n",
            "3              ...                                          \n",
            "4              ...                                          \n",
            "\n",
            "[5 rows x 169 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFxA-4EsqXKo",
        "outputId": "2de066b7-a2a1-483e-968d-b2a3409226a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RangeIndex(start=0, stop=169, step=1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set column names based on the first row\n",
        "df.columns = df.iloc[0]\n",
        "\n",
        "# Drop the first row, which contains column names\n",
        "df = df[1:]\n"
      ],
      "metadata": {
        "id": "upnQTCRlUcxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values (e.g., replace NaN with an empty string for text data)\n",
        "# Assuming the column containing resume text data is named 'Resume_str'\n",
        "df['Resume_str'].fillna('', inplace=True)\n",
        "\n",
        "# Data cleaning (if needed)\n",
        "# For example, you can drop irrelevant columns\n",
        "# df.drop(columns=['Column1', 'Column2'], inplace=True)\n",
        "\n",
        "# Text preprocessing\n",
        "# Lowercase text, tokenize, and remove stopwords and special characters\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenize text\n",
        "    tokens = [word for word in tokens if word.isalnum()]  # Remove special characters\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['Cleaned_Resume'] = df['Resume_str'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idtlH1D849Sc",
        "outputId": "dcd513d7-0073-4011-bb64-bdfe09717d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values (e.g., replace NaN with an empty string for text data)\n",
        "# Assuming the column containing resume text data is named 'Resume_str'\n",
        "df['Resume_str'].fillna('', inplace=True)\n",
        "\n",
        "# Data cleaning (if needed)\n",
        "# For example, you can drop irrelevant columns\n",
        "# df.drop(columns=['Column1', 'Column2'], inplace=True)\n",
        "\n",
        "# Text preprocessing\n",
        "# Lowercase text, tokenize, and remove stopwords and special characters\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenize text\n",
        "    tokens = [word for word in tokens if word.isalnum()]  # Remove special characters\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['Cleaned_Resume'] = df['Resume_str'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6ptgYXeDL0O",
        "outputId": "f6781e79-d64d-446b-8de4-c5818bd38ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI-sfC2ZDurx",
        "outputId": "feeaeec3-b1e8-4b02-aad2-2a858ec8492a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        ID                                         Resume_str  \\\n",
            "1  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
            "2  22323967           HR SPECIALIST, US HR OPERATIONS      ...   \n",
            "3  33176873           HR DIRECTOR       Summary      Over 2...   \n",
            "4  27018550           HR SPECIALIST       Summary    Dedica...   \n",
            "5  17812897           HR MANAGER         Skill Highlights  ...   \n",
            "\n",
            "0                                        Resume_html Category              \\\n",
            "1  <div class=\"fontsize fontface vmargins hmargin...       HR               \n",
            "2  <div class=\"fontsize fontface vmargins hmargin...       HR               \n",
            "3  <div class=\"fontsize fontface vmargins hmargin...       HR               \n",
            "4  <div class=\"fontsize fontface vmargins hmargin...       HR               \n",
            "5  <div class=\"fontsize fontface vmargins hmargin...       HR               \n",
            "\n",
            "0  ...                                                       Cleaned_Resume  \n",
            "1  ...                    hr associate hr administrator summary dedicate...  \n",
            "2  ...                    hr specialist us hr operations summary versati...  \n",
            "3  ...                    hr director summary 20 years experience recrui...  \n",
            "4  ...                    hr specialist summary dedicated driven dynamic...  \n",
            "5  ...                    hr manager skill highlights hr skills hr depar...  \n",
            "\n",
            "[5 rows x 170 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Word Count\n",
        "df['Word_Count'] = df['Cleaned_Resume'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Average Word Length\n",
        "df['Avg_Word_Length'] = df['Cleaned_Resume'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf-1kIktFfAB",
        "outputId": "234eeea8-7999-42e5-bf38-334add6f3220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, lowercase=True, stop_words='english')\n",
        "\n",
        "# Fit and transform the text data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Cleaned_Resume'])\n",
        "\n",
        "# Convert TF-IDF matrix to DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Join the TF-IDF DataFrame with the original DataFrame\n",
        "df = pd.concat([df, tfidf_df], axis=1)\n"
      ],
      "metadata": {
        "id": "8x_Du6QoHDy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values in 'Cleaned_Resume' by filling with an empty string\n",
        "df['Cleaned_Resume'].fillna('', inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define a function to get sentiment scores\n",
        "def get_sentiment_scores(text):\n",
        "    sentiment = sia.polarity_scores(text)\n",
        "    return sentiment\n",
        "\n",
        "# Apply sentiment analysis to each resume and create new columns for sentiment scores\n",
        "df['Sentiment_Scores'] = df['Cleaned_Resume'].apply(get_sentiment_scores)\n",
        "\n",
        "# Extract individual sentiment scores\n",
        "df['Sentiment_Positive'] = df['Sentiment_Scores'].apply(lambda x: x['pos'])\n",
        "df['Sentiment_Negative'] = df['Sentiment_Scores'].apply(lambda x: x['neg'])\n",
        "df['Sentiment_Neutral'] = df['Sentiment_Scores'].apply(lambda x: x['neu'])\n",
        "df['Sentiment_Compound'] = df['Sentiment_Scores'].apply(lambda x: x['compound'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbEY5BJ6HphO",
        "outputId": "0aa24fe2-e6d7-49e8-bc21-453a7d3961ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ],
      "metadata": {
        "id": "EGICgFBC8yCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create a scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the numerical columns\n",
        "df[['Word_Count', 'Avg_Word_Length']] = scaler.fit_transform(df[['Word_Count', 'Avg_Word_Length']])\n"
      ],
      "metadata": {
        "id": "_WBQSTNMDKkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('Cleaned_Resume', axis=1)\n",
        "\n",
        "# Select the features you want to include in X_train and X_val\n",
        "# Handle missing values in 'Cleaned_Resume' by filling with an empty string\n",
        "df['Cleaned_Resume'].fillna('', inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "output\n",
        "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
        "[ ]\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "[ ]\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create a scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the numerical columns\n",
        "df[['Word_Count', 'Avg_Word_Length']] = scaler.fit_transform(df[['Word_Count', 'Avg_Word_Length']])\n",
        "\n",
        "[ ]\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('Cleaned_Resume', axis=1)\n",
        "\n",
        "# Select the features you want to include in X_train and X_val\n",
        "selected_features = ['Word_Count', 'Avg_Word_Length', 'Sentiment_Positive', 'Sentiment_Negative', 'Sentiment_Neutral', 'Sentiment_Compound']\n",
        "\n",
        "# Construct X_train and X_val using the selected features\n",
        "X_train, X_val = train_test_split(df[selected_features], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "[ ]\n",
        "#////////////////DO NOT RUN THIS (BACKUP)//////////////\n",
        "# Select the features you want to include in X_train and X_val\n",
        "selected_features = ['Word_Count', 'Avg_Word_Length', 'Sentiment_Positive', 'Sentiment_Negative', 'Sentiment_Neutral', 'Sentiment_Compound']\n",
        "\n",
        "# Add TF-IDF features\n",
        "selected_features.extend(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Construct X_train and X_val using the selected features\n",
        "X_train, X_val = train_test_split(df[selected_features], test_size=0.2, random_state=42)\n",
        "\n",
        "[ ]\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the dimensions for the input and encoded representations\n",
        "input_dim = X_train.shape[1]\n",
        "encoding_dim = 128  # You can adjust this value as needed\n",
        "\n",
        "# Input layer\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "\n",
        "# Encoder layers\n",
        "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "\n",
        "# Decoder layers\n",
        "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "# Create the autoencoder model\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "[ ]\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32)\n",
        "\n",
        "output\n",
        "Epoch 1/50\n",
        "9/9 [==============================] - 1s 2ms/step - loss: nan\n",
        "Epoch 2/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 3/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 4/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 5/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 6/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 7/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 8/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 9/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 10/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 11/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 12/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 13/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 14/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 15/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 16/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 17/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 18/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 19/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 20/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 21/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 22/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 23/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 24/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 25/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 26/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 27/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 28/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 29/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 30/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 31/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 32/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 33/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 34/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 35/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 36/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 37/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 38/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 39/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 40/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 41/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 42/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 43/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 44/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 45/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "Epoch 46/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 47/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 48/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 49/50\n",
        "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
        "Epoch 50/50\n",
        "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
        "<keras.src.callbacks.History at 0x7ba538c87430>\n",
        "[ ]\n",
        "\n",
        "New Section\n",
        "Colab paid products - Cancel contracts here\n",
        "\n",
        "selected_features = ['Word_Count', 'Avg_Word_Length', 'Sentiment_Positive', 'Sentiment_Negative', 'Sentiment_Neutral', 'Sentiment_Compound']\n",
        "\n",
        "# Construct X_train and X_val using the selected features\n",
        "X_train, X_val = train_test_split(df[selected_features], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S4cgjePZDSek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#////////////////DO NOT RUN THIS (BACKUP)//////////////\n",
        "# Select the features you want to include in X_train and X_val\n",
        "selected_features = ['Word_Count', 'Avg_Word_Length', 'Sentiment_Positive', 'Sentiment_Negative', 'Sentiment_Neutral', 'Sentiment_Compound']\n",
        "\n",
        "# Add TF-IDF features\n",
        "selected_features.extend(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Construct X_train and X_val using the selected features\n",
        "X_train, X_val = train_test_split(df[selected_features], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "YNw0bT6z65Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the dimensions for the input and encoded representations\n",
        "input_dim = X_train.shape[1]\n",
        "encoding_dim = 128  # You can adjust this value as needed\n",
        "\n",
        "# Input layer\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "\n",
        "# Encoder layers\n",
        "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "\n",
        "# Decoder layers\n",
        "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "# Create the autoencoder model\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n"
      ],
      "metadata": {
        "id": "PygByjp1DtuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the autoencoder\n",
        "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNaClKGS64AO",
        "outputId": "d619fca8-bb87-47d9-d8fb-35e67b5709da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "9/9 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 2/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 3/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 4/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 5/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 6/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 7/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 8/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 9/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 10/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 11/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 12/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 13/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 14/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 15/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 16/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 17/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 18/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 19/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 20/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 21/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 22/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 23/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 24/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 25/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 26/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 27/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 28/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 29/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 30/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 31/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 32/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 33/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 34/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 35/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 36/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 37/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 38/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 39/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 40/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 41/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 42/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 43/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 44/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 45/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 46/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 47/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 48/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 49/50\n",
            "9/9 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 50/50\n",
            "9/9 [==============================] - 0s 1ms/step - loss: nan\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ba538c87430>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjK-Lis9JKvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "rknzrvd81qNM"
      }
    }
  ]
}